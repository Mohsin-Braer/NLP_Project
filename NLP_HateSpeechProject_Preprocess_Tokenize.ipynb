{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UbwTfg9Ochj-",
        "HAKp-EyYdrrv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the data label is the 'class' field:\n",
        "\n",
        "  0 -> Hate Speech\n",
        "\n",
        "  1 -> Offensive Language\n",
        "\n",
        "  2 -> Neither"
      ],
      "metadata": {
        "id": "NZXQomP1f3dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4w56jiTzCCSA"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import google drive"
      ],
      "metadata": {
        "id": "UbwTfg9Ochj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovzoFxP6aWpM",
        "outputId": "66687cf8-73cc-4d64-f394-32e368fc7fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "hatespeech_data.csv  hatespeech_data_tokenized.csv\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/MyDrive/escola/nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put data into pandas"
      ],
      "metadata": {
        "id": "HAKp-EyYdrrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/escola/nlp/hatespeech_data.csv\") "
      ],
      "metadata": {
        "id": "3WcE7Jfhdrz9"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ],
      "metadata": {
        "id": "rOg81vqG47i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every tweet: lowercase, remove urls, change @... and #... to 'TAG', remove 'rt', remove non-letter characters, strip leading and trailing white space. Create a list with all the words."
      ],
      "metadata": {
        "id": "vE0TOHNK8_2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences = []\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "  tweet = \"\"\n",
        "  s = df.iloc[i]['tweet']\n",
        "  # lowercase\n",
        "  s = s.lower()\n",
        "  # remove urls\n",
        "  s = re.sub(r'https?:\\/\\/\\S+', ' ', s)\n",
        "  s = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ', s)\n",
        "  # change @... and #abc to thisisatag\n",
        "  s = re.sub(r'\\s([@#][\\w_-]+)', ' thisisatag ', s)\n",
        "  # remove non-letter chars\n",
        "  s = re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", ' ', s)\n",
        "  # thisisatag -> TAG\n",
        "  s = re.sub(r\" thisisatag \", ' TAG ', s)\n",
        "  # remove rt\n",
        "  s = re.sub(r\" rt \", ' ', s)\n",
        "  # remove unecessary spacing\n",
        "  s = s.strip()\n",
        "  s = tweet_tokenizer.tokenize(s)\n",
        "  # remove punctuation after tokenization\n",
        "  for j in s:\n",
        "    if j not in string.punctuation:\n",
        "      tweet = tweet + j + \" \"\n",
        "  all_sentences.append(tweet[:-1]) "
      ],
      "metadata": {
        "id": "8dtgqRDF95w4"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Dataset"
      ],
      "metadata": {
        "id": "R6Yk6ZO3oAq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text_df = pd.DataFrame(columns=['tweet', 'label'])\n",
        "for i in range(len(all_sentences)):\n",
        "  # add tweet and label to clean_df\n",
        "  tweet = all_sentences[i]\n",
        "  label = df.iloc[i]['class']\n",
        "  # is hate speech\n",
        "  if label == 0:\n",
        "    label = 1\n",
        "  # not hate speech \n",
        "  else:\n",
        "    label = 0\n",
        "  clean_text_df.loc[i] = [tweet, label]"
      ],
      "metadata": {
        "id": "4SWAiOIfoEce"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Text to Numbers"
      ],
      "metadata": {
        "id": "ILgDdKYxEy9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myTokenizer = Tokenizer()\n",
        "myTokenizer.fit_on_texts(all_sentences)\n",
        "token_sequences = myTokenizer.texts_to_sequences(all_sentences)\n",
        "padded_sequences = pad_sequences(token_sequences)"
      ],
      "metadata": {
        "id": "6pJrw_DmFf86"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating data ready for training\n"
      ],
      "metadata": {
        "id": "nT7VfPFNF-UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = pd.DataFrame(columns=['tweet', 'label'])\n",
        "for i in range(len(padded_sequences)):\n",
        "  # add tweet and label to clean_df\n",
        "  clean_df.loc[i] = [padded_sequences[i].tolist(), df.iloc[i]['class']]"
      ],
      "metadata": {
        "id": "og7Qw5PVF9nM"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting it as a CSV"
      ],
      "metadata": {
        "id": "cc85ZwD3K4U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text_df.to_csv(\"/content/drive/MyDrive/escola/nlp/hatespeech_data_clean_text.csv\")"
      ],
      "metadata": {
        "id": "vNMjVINNonRv"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.to_csv(\"/content/drive/MyDrive/escola/nlp/hatespeech_data_tokenized.csv\")"
      ],
      "metadata": {
        "id": "U_WoecnuK8E1"
      },
      "execution_count": 146,
      "outputs": []
    }
  ]
}